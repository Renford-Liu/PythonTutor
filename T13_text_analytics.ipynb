{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析的基本概念\n",
    "\n",
    "跟我们之前介绍的分析不同，文本数据是典型的非结构化数据，而且本身具有非常复杂的结构，相对之前的数据分析相比有其特有的分析技巧。这里我们将简单介绍文本分析的基本流程以及一些基础的工具。\n",
    "\n",
    "虽然差别很大，但是基本流程与之前的分析还是想通的，一般都需要如下步骤：\n",
    "\n",
    "1. 准备数据，在这一步除了要准备需要进行分析的数据之外，可能还需要准备额外的语料库（corpus）。\n",
    "2. 文本规范化处理，也就是我们之前清洗数据的步骤，比如分词、去除停用词、去除特殊符号等无意义字符、同义词转换、缩写转换等等。\n",
    "3. 特征工程，从已经清洗好的数据中提取特征。由于计算机只能处理数值型的变量，因而在这一步有一个比较关键的步骤是将文字转换为计算机可以理解的向量等数值型变量。\n",
    "4. 训练模型，经过这些步骤后，针对不同的目的，模型训练可能与之前的算法比较类似，但是也有针对文本数据特有的模型。\n",
    "5. 模型评价，评价模型的性能，重复以上步骤，改进模型。\n",
    "\n",
    "针对文本数据，除了其他数据同样可以进行的相关性计算、聚类、分类等模型之外，还有一些任务是文本数据特有的，比如：\n",
    "\n",
    "* 分词\n",
    "* 词性标注\n",
    "* 词嵌入\n",
    "* 摘要和主题建模\n",
    "* 实体识别\n",
    "* 知识图谱\n",
    "* 语义分析\n",
    "* ......\n",
    "\n",
    "其中有的模型结果是其他模型的基础，比如分词、词性标注等是很多其他模型的基础。\n",
    "\n",
    "在本节，我们将主要使用Python中的**NLTK**（http://www.nltk.org ）、**Scikit-Learn**、**Jieba**（https://github.com/fxsjy/jieba ）、**Gensim**（https://github.com/RaRe-Technologies/gensim ）等工具介绍文本分析的基本原理。不过与此同时，自然语言处理，包括中文的自然语言处理正在蓬勃发展，很多新的工具可以使用，比如对标NLTK并且号称有更好性能的**spaCy**（ https://spacy.io ），以及已经经过预训练可以直接拿来用的模型比如最近如火如荼的**BERT**（）、**HanLP**（https://github.com/hankcs/HanLP ）、**Stanford CoreNLP**（https://github.com/stanfordnlp/CoreNLP ）等等，学习基本原理后可以直接使用这些包进行自己的研究。\n",
    "\n",
    "我们首先从文本的规范化处理开始，介绍文本分析的基本原理和方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本规范化\n",
    "\n",
    "文本是典型的非结构化数据，我们需要将文本转换为高度结构化的数据，首先要对文本进行有意义的划分，一般涉及到句子的**切分**（**tokenization**）以及其他清洗步骤。\n",
    "\n",
    "**标识**（**token**）是文本的有意义的最小成分，文本处理的最简单操作即将文本切成一个个的token，通常包括句子切分和词语切分。接下来我们很少有研究句子的成分和语义，更多时候是针对词语的分析，因而接下来主要介绍句子的切分方法。\n",
    "\n",
    "## 英文切分\n",
    "\n",
    "英文的词语切分一般比较简单，主要原因是因为英文的单词之间都有空格进行分割，而中文的切分就复杂很多。常见的自然语言处理包比如NKTL以及spaCy都肯定包含了切分的函数，比如在NLTK中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State', '.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', ',', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', '.', 'Fake', 'News', 'that', 'I', 'won', '’', 't', 'help', 'them', 'because', 'I', 'don', '’', 't', 'like', 'Cuomo', '(', 'I', 'do', ')', '.', 'Just', 'sent', '4000', 'ventilators', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words=word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意运行以上命令可能先要下载相应的包：在Python解释器中运行：nltk.download('punkt') ，如果提示错误，可以参考：https://www.cnblogs.com/sddai/p/10543359.html\n",
    "\n",
    "当然NLTK不止支持这一种切分方法，比如我们可以使用正则表达式切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', 'State', 'Dealing', 'with', 'both', 'Mayor', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', 'I', 'do', 'Just', 'sent', '4000', 'ventilators']\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "Tokenizer=RegexpTokenizer(pattern=r\"[\\w\\-’']+\")\n",
    "words=Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还可以使用空白字符（空格、缩进、换行）等进行切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', '(I', 'do).', 'Just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "Tokenizer=WhitespaceTokenizer()\n",
    "words=Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文分词虽然原理简单，但是还是有很多细节的坑，比如上面Trump先生的「don’t」和「don't」，如果在切分或者其他清洗步骤中予以重视，计算机会认为这是两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词\n",
    "\n",
    "中文文本分析与英文的文本分析一个最重要的区别在于，英文使用空格分割每个单词，但是中文没有分割单词的概念。\n",
    "\n",
    "为了克服这个问题，分词就应运而生了。结合字典和算法，分词软件可以帮助我们将中文的文章、句子分解为一个个的中文单词。\n",
    "\n",
    "目前已经有很多成熟的分词工具，比如中科院的NLPIR汉语分词系统、结巴分词以及腾讯、阿里、百度的分词系统等等。在这里我们以开源的结巴分词为例，介绍分词工具的用法。\n",
    "\n",
    "为了使用结巴分词，首先需要安装。在terminal中输入：\n",
    "```shell\n",
    "pip install jieba\n",
    "```\n",
    "\n",
    "就可以进行安装了。安装好之后，可以将jieba模块导入到Python程序中，就可以正常使用了：\n",
    "```shell\n",
    "import jieba\n",
    "```\n",
    "\n",
    "比如，最简单的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.508 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "line=\"今年以来，我国持续推进减税降费、提高最低工资标准、促进就业，特别是年初开始实施的个人所得税改革以及专项附加扣除方案，有效增加了居民可支配收入。与此同时，不断消除居民消费的后顾之忧。消费需求进一步释放，消费市场亮点纷呈\"\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut()函数有三个参数：必须要提供的是需要进行分词的字符串；此外，cut_all参数控制是否采用全模式；HMM参数用来控制是否使用HMM模型。其区别是：\n",
    "\n",
    "* cut_all=True， 代表使用全模式，全模式可以切出混合不同粒度的词\n",
    "* HMM=True，代表使用HMM模型，用于推断字典中没有的词\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面的分词结果......一言难尽。实际上，任何分词算法都不可避免的不能跟上时代的潮流，特别是网络时代，新的词语层出不穷，而在一些专业领域中，一些专有名词往往普通词典无法完全覆盖。比如“减费降税”、“可支配收入”这些专有名词，都没有被正确分出来。\n",
    "\n",
    "为了克服这个问题，往往需要用户自己添加字典。比如，我们可以把“减费降税”、“可支配收入”这些名词放在一个文本文件中，每个新词写成一行，然后使用load_userdict()函数给定这个文件，就可以添加自己的新词列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('user_dict.txt','wt') as f:\n",
    "    f.write(\"减税降费\\n可支配收入\\n最低工资\\n最低工资标准\\n专项附加扣除\\n消费需求\")\n",
    "jieba.load_userdict('user_dict.txt')\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入了用户字典后，新的分词更加准确了。在实际应用的时候，无论使用什么分词工具，用户字典的构建往往是非常关键的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大小写转换\n",
    "\n",
    "主要针对英文等字母文字，一般的做法是统一转换为小写字母，避免出现「FAKE NEWS$\\neq$fake news」的情况出现。可以使用字符串的.lower()方法很容易的完成大小写转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'working', 'very', 'hard', 'to', 'help', 'new', 'york', 'city', '&', 'state.', 'dealing', 'with', 'both', 'mayor', '&', 'governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'fake', 'news', 'that', 'i', 'won’t', 'help', 'them', 'because', 'i', 'don’t', 'like', 'cuomo', '(i', 'do).', 'just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "words=[w.lower() for w in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除停用词和特殊字符\n",
    "\n",
    "分词之后，在进一步进行处理之前，消除停用词和特殊符号往往是非常关键的。比如在上面的分词结果中，“的”、“是”、“与此同时”等，含义并不是非常明显，对其分析的意义不大，留着这些词只会空占内存，并且可能对分析结果产生巨大影响。一个常用的做法是，使用一个停用词列表，分词结束之后，把停用词列表中的词全都剔除出去。\n",
    "\n",
    "比如，在文本文档“chinese_cutting/stopword.txt”中，我们列举出了一些常用的停用词以及特殊字符，我们可以使用如下方法消除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=set([w.lower() for w in stoplist])\n",
    "wlist=jieba.cut(line)\n",
    "wlist=[w.lower() for w in wlist if w not in stoplist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意上面的函数里面我们还同时对停用词和词语转换为了小写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩展缩写词、同义词转换\n",
    "\n",
    "缩写词即诸如：「isn't==is not」之类的缩写，一般而言也需要特殊处理。一般来说我们可以通过定义一个映射关系来处理这种情况，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "syno={\"isn't\": \"is not\",\n",
    "     \"aren't\": \"are not\",\n",
    "     \"i'll\": \"i will\",\n",
    "     \"个人所得税\":\"个税\"}\n",
    "wlist=[syno[w]  if w in syno.keys() else w for w in wlist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还有一些更加细致的工作，比如检查拼写错误、矫正重复字符等等，此外英文可能还涉及到时态、单复数的问题，这些都需要大量细致的工作，特别是针对不同的应用场景进行特定的优化是非常有必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个简单的例子：中文词频统计\n",
    "\n",
    "以下实现了对《越女剑》的词频统计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>范蠡</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>道</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剑士</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>青衣</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿青</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>勾践</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>锦衫</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>长剑</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>说道</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>吴国</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency\n",
       "0   范蠡        118\n",
       "1    道        113\n",
       "2   剑士        105\n",
       "3   青衣         47\n",
       "4   阿青         47\n",
       "5   勾践         44\n",
       "6   锦衫         35\n",
       "7   长剑         34\n",
       "8   说道         31\n",
       "9   吴国         30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "## 停用词\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.lower().strip() for w in stoplist]\n",
    "## 读入小说\n",
    "wordlist=[]\n",
    "with open('Chinese_cutting/越女剑.txt','rt') as f:\n",
    "    for l in f:\n",
    "        line_cut=jieba.cut(l)\n",
    "        line=[w.strip().lower() for w in line_cut]\n",
    "        wordlist.extend(line)\n",
    "wordlist=[w for w in wordlist if w not in stoplist]\n",
    "## 统计\n",
    "text_dict=dict()\n",
    "for l in wordlist:\n",
    "    if l not in text_dict:\n",
    "        text_dict[l]=1\n",
    "    else:\n",
    "        text_dict[l]+=1\n",
    "text_freq=[]\n",
    "for k in text_dict:\n",
    "    text_freq.append((k,text_dict[k]))\n",
    "## 排序\n",
    "text_freq.sort(key=lambda x: x[1],reverse=True)\n",
    "## 用pandas显示，更好看\n",
    "import pandas as pd\n",
    "freq=pd.DataFrame(text_freq, columns=['word','frequency'])\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取特征\n",
    "\n",
    "由于计算机只能处理数值变量进行运算，在文本数据清洗完毕后，接下来通常需要将其转换为计算机可以识别的向量。目前有多重方法可以完成这项任务，我们在这里主要介绍其中常见的三种：词袋、TF-IDF以及词嵌入三种模型。\n",
    "\n",
    "## 词袋模型\n",
    "\n",
    "**词袋**（**bag of words**）模型是最基础的一种将文本数据结构化为向量的一种方法。实际上词袋可以简单理解为每个文本的词频统计。比如，我们接下来使用一些上市公司的标题数据，并使用上面介绍的方法将每个标题转换为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59021772</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59021779</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date                                          title\n",
       "0  59021767 2011-02-19                                   沪争取增值税扩围改革试点\n",
       "1  59021769 2011-02-19                            周小川：外部施压不会影响人民币升值步伐\n",
       "2  59021771 2011-02-19                             准备金率再上调 达到19.5％创新高\n",
       "3  59021772 2011-02-19  浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "4  59021774 2011-02-19                            定基价格指数若涨20% 政府或出手调控\n",
       "5  59021776 2011-02-19                          政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6  59021778 2011-02-19                     新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "7  59021779 2011-02-19    沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "8  59021783 2011-02-19                               北京：楼市限购首日成交环比降9成\n",
       "9  59021787 2011-02-19                            上海：房管局发布“沪九条”限购执行细则"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RAW=pd.read_csv(\"csv/stocknews1.csv\")\n",
    "RAW['date']=pd.to_datetime(RAW['date'])\n",
    "RAW.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们额外进行一些处理，注意到其中的类似「2011年02月14日2011年02月18日二级市场表现周简报」之类的title很没有营养，我们打算去除他，可以使用正则表达式方便的达到目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134769</th>\n",
       "      <td>19839999</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>定价基准不同 新利率基准将冲击旧浮动利率债</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134770</th>\n",
       "      <td>19840000</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>债市延续调整格局 投资者宜缩短投资久期</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134771</th>\n",
       "      <td>19840023</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>曾培炎:利用外汇储备优势</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134772</th>\n",
       "      <td>19840026</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>广州小时最低工资标准7.5元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134773</th>\n",
       "      <td>19840032</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>左小蕾:警惕股市系统性风险</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>905013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       date                  title\n",
       "0        59021767 2011-02-19           沪争取增值税扩围改革试点\n",
       "1        59021769 2011-02-19    周小川：外部施压不会影响人民币升值步伐\n",
       "2        59021771 2011-02-19     准备金率再上调 达到19.5％创新高\n",
       "4        59021774 2011-02-19    定基价格指数若涨20% 政府或出手调控\n",
       "5        59021776 2011-02-19  政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "...           ...        ...                    ...\n",
       "1134769  19839999 2006-12-27  定价基准不同 新利率基准将冲击旧浮动利率债\n",
       "1134770  19840000 2006-12-27    债市延续调整格局 投资者宜缩短投资久期\n",
       "1134771  19840023 2006-12-27           曾培炎:利用外汇储备优势\n",
       "1134772  19840026 2006-12-27         广州小时最低工资标准7.5元\n",
       "1134773  19840032 2006-12-27         左小蕾:警惕股市系统性风险 \n",
       "\n",
       "[905013 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jianbao=RAW['title'].str.match(r'.+\\d{4}年\\d{2}月\\d{2}日.+简报')\n",
    "RAW1=RAW.iloc[list(~jianbao),:]\n",
    "RAW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便起见，我们选取其中的前十条先进行分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59021789</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>6.5781！人民币升值容忍度继续提高？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59021791</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>2010年上海商品住宅销售降四成</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       date                       title\n",
       "0   59021767 2011-02-19                沪争取增值税扩围改革试点\n",
       "1   59021769 2011-02-19         周小川：外部施压不会影响人民币升值步伐\n",
       "2   59021771 2011-02-19          准备金率再上调 达到19.5％创新高\n",
       "4   59021774 2011-02-19         定基价格指数若涨20% 政府或出手调控\n",
       "5   59021776 2011-02-19       政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6   59021778 2011-02-19  新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "8   59021783 2011-02-19            北京：楼市限购首日成交环比降9成\n",
       "9   59021787 2011-02-19         上海：房管局发布“沪九条”限购执行细则\n",
       "10  59021789 2011-02-19        6.5781！人民币升值容忍度继续提高？\n",
       "11  59021791 2011-02-19            2010年上海商品住宅销售降四成"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=RAW1.iloc[:10,:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '19.5', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '20%', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '13', '地州', '探矿权', '年', '590', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['6.5781', '人民币', '升值', '容忍度', '提高'],\n",
       " ['2010', '年', '上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 首先进行分词、去除停用词等\n",
    "import jieba\n",
    "\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if w not in stoplist and len(w.strip())>0]\n",
    "    return cut_w\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 65 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 接下来进行词频统计\n",
    "\n",
    "def word_freq(wlist):\n",
    "    freq={}\n",
    "    for w in wlist:\n",
    "        if w in freq:\n",
    "            freq[w]+=1\n",
    "        else:\n",
    "            freq[w]=1\n",
    "            \n",
    "    return freq\n",
    "freqs=map(word_freq,tokenized_data)\n",
    "## 放在pandas里\n",
    "pd_freqs=pd.DataFrame(freqs)\n",
    "## 把NaN换成0\n",
    "pd_freqs=pd_freqs.fillna(0)\n",
    "pd_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，一个简单的词袋模型就完成了。\n",
    "\n",
    "不过，可以看到虽然我们只有10条新闻，这个矩阵已经很大了。如果10万条新闻一起来，不仅词（列）多，而且行也多，最终这个矩阵的规模会变的非常巨大，甚至可能很轻易的会占满内存。\n",
    "\n",
    "然而注意到，这个矩阵里面多数的值都是0，这种类型的矩阵我们成为**稀疏矩阵**（**sparse matrix**），在SciPy和Pandas里面都提供了稀疏矩阵的存储结构和运算，所以更好的办法是使用稀疏矩阵进行存储："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 65 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs=pd_freqs.astype(pd.SparseDtype(\"float\", 0))\n",
    "sparse_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的意思是通过类型转换，把数据框转换为稀疏类型，其中的「0」就不存储了，虽然看起来没有变化，但是如果我们查看类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "沪       Sparse[float64, 0]\n",
       "增值税     Sparse[float64, 0]\n",
       "扩围      Sparse[float64, 0]\n",
       "改革      Sparse[float64, 0]\n",
       "试点      Sparse[float64, 0]\n",
       "               ...        \n",
       "2010    Sparse[float64, 0]\n",
       "商品住宅    Sparse[float64, 0]\n",
       "销售      Sparse[float64, 0]\n",
       "降       Sparse[float64, 0]\n",
       "四成      Sparse[float64, 0]\n",
       "Length: 65, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会发现都变成了Sparse的float64类型，且0不存储。\n",
    "\n",
    "当然，以上是自己手写的词袋生成步骤，实际上很多自然语言处理包已经有比较成熟的词袋处理机制。比如在Scikit-Learn中已经准备好了词袋的提取函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10x66 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer(tokenizer=jieba.cut, stop_words=stoplist, min_df=1)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意由于Scikit-Learn中一般只支持英文的自动分词（tokenize），为了让他能够处理中文的分词，我们把jieba.cut函数提交给了CountVectorizer，此外还额外提供了停用词列表。min_df选项设置了如果在所有文本中某个词出现的频率下线，如果出现太少则会被忽略，适当提高这个选项可以降低维数。\n",
    "\n",
    "上面的词袋结果是一个sparse matrix，实际上是SciPy中的系数矩阵形式，实际分析时已经可以使用。不过为了查看方便，我们不妨将其转换为Pandas的数据框（稀疏存储）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>19.5</th>\n",
       "      <th>20%</th>\n",
       "      <th>2010</th>\n",
       "      <th>590</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      13  19.5  20%  2010  590  6.5781  上海  上调  不探  ...  藏药  西藏  试点  调控  销售  \\\n",
       "0  0   0     0    0     0    0       0   0   0   0  ...   0   0   1   0   0   \n",
       "1  0   0     0    0     0    0       0   0   0   0  ...   0   0   0   0   0   \n",
       "2  1   0     1    0     0    0       0   0   1   0  ...   0   0   0   0   0   \n",
       "3  1   0     0    1     0    0       0   0   0   0  ...   0   0   0   1   0   \n",
       "4  1   0     0    0     0    0       0   0   0   0  ...   1   1   0   0   0   \n",
       "5  1   1     0    0     0    1       0   0   0   1  ...   0   0   0   0   0   \n",
       "6  0   0     0    0     0    0       0   0   0   0  ...   0   0   0   0   0   \n",
       "7  0   0     0    0     0    0       0   1   0   0  ...   0   0   0   0   0   \n",
       "8  0   0     0    0     0    0       1   0   0   0  ...   0   0   0   0   0   \n",
       "9  0   0     0    0     1    0       0   1   0   0  ...   0   0   0   0   1   \n",
       "\n",
       "   降  限购  项目  首日  高  \n",
       "0  0   0   0   0  0  \n",
       "1  0   0   0   0  0  \n",
       "2  0   0   0   0  1  \n",
       "3  0   0   0   0  0  \n",
       "4  0   0   0   0  0  \n",
       "5  0   0   1   0  0  \n",
       "6  0   1   0   1  0  \n",
       "7  0   1   0   0  0  \n",
       "8  0   0   0   0  0  \n",
       "9  1   0   0   0  0  \n",
       "\n",
       "[10 rows x 66 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也许你会觉着不舒服的是，分词和去除停用词等操作应该是第二步清洗数据完成的，现在如果都放到向量化的对象CountVectorizer中来，非常不灵活。\n",
    "\n",
    "比如显然上面的结果中，「13，19.5，20%」等都不是我们想要的，然而用停用词根本不可能将这些数字去除。\n",
    "\n",
    "那么如何将两者（手工清洗+自动计算词袋）结合起来呢？其实很简单，用空格将他们join起来就好了。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['沪 增值税 扩围 改革 试点',\n",
       " '周小川 外部 施压 影响 人民币 升值 步伐',\n",
       " '准备金率 上调 创新 高',\n",
       " '定基 价格指数 若涨 政府 出手 调控',\n",
       " '政策 倾斜 加大 投入 西藏 做 做 强 藏药 产业',\n",
       " '新疆 地州 探矿权 年 项目 涉嫌 圈 不探',\n",
       " '北京 楼市 限购 首日 成交 环 比降 成',\n",
       " '上海 房管局 发布 沪 九条 限购 执行 细则',\n",
       " '人民币 升值 容忍度 提高',\n",
       " '年 上海 商品住宅 销售 降 四成']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def not_digit(w):\n",
    "    w=w.replace(',','')\n",
    "    if re.match(r'\\d+',w)!=None or re.match(r'\\d%',w)!=None or re.match(r'\\d*\\.\\d+',w)!=None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if ((w not in stoplist) and not_digit(w) and len(w.strip())>0)]\n",
    "    return cut_w\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=[' '.join(t) for t in tokenized_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>出手</th>\n",
       "      <th>...</th>\n",
       "      <th>细则</th>\n",
       "      <th>若涨</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  准备金率  出手  ...  细则  若涨  藏药  西藏  试点  调控  \\\n",
       "0   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   1   0   \n",
       "1   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "2   0   1   0   0   0    0     0   0     1   0  ...   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0    0     1   0     0   1  ...   0   1   0   0   0   1   \n",
       "4   0   0   0   0   1    0     0   1     0   0  ...   0   0   1   1   0   0   \n",
       "5   0   0   1   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "6   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "7   1   0   0   1   0    0     0   0     0   0  ...   1   0   0   0   0   0   \n",
       "8   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "9   1   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "   销售  限购  项目  首日  \n",
       "0   0   0   0   0  \n",
       "1   0   0   0   0  \n",
       "2   0   0   0   0  \n",
       "3   0   0   0   0  \n",
       "4   0   0   0   0  \n",
       "5   0   0   1   0  \n",
       "6   0   1   0   1  \n",
       "7   0   1   0   0  \n",
       "8   0   0   0   0  \n",
       "9   1   0   0   0  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer()\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，更优雅的方法是直接将手写的tokenize函数调入即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 59 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上以上的词袋又被称为1元词袋，是N元（N-gram）词袋的一种特例。我们当然可以做成二元词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海 商品住宅</th>\n",
       "      <th>上海 房管局</th>\n",
       "      <th>上调 创新</th>\n",
       "      <th>九条 限购</th>\n",
       "      <th>人民币 升值</th>\n",
       "      <th>价格指数 若涨</th>\n",
       "      <th>倾斜 加大</th>\n",
       "      <th>做 做</th>\n",
       "      <th>做 强</th>\n",
       "      <th>准备金率 上调</th>\n",
       "      <th>...</th>\n",
       "      <th>环 比降</th>\n",
       "      <th>若涨 政府</th>\n",
       "      <th>藏药 产业</th>\n",
       "      <th>西藏 做</th>\n",
       "      <th>销售 降</th>\n",
       "      <th>降 四成</th>\n",
       "      <th>限购 执行</th>\n",
       "      <th>限购 首日</th>\n",
       "      <th>项目 涉嫌</th>\n",
       "      <th>首日 成交</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海 商品住宅  上海 房管局  上调 创新  九条 限购  人民币 升值  价格指数 若涨  倾斜 加大  做 做  做 强  准备金率 上调  \\\n",
       "0        0       0      0      0       0        0      0    0    0        0   \n",
       "1        0       0      0      0       1        0      0    0    0        0   \n",
       "2        0       0      1      0       0        0      0    0    0        1   \n",
       "3        0       0      0      0       0        1      0    0    0        0   \n",
       "4        0       0      0      0       0        0      1    1    1        0   \n",
       "5        0       0      0      0       0        0      0    0    0        0   \n",
       "6        0       0      0      0       0        0      0    0    0        0   \n",
       "7        0       1      0      1       0        0      0    0    0        0   \n",
       "8        0       0      0      0       1        0      0    0    0        0   \n",
       "9        1       0      0      0       0        0      0    0    0        0   \n",
       "\n",
       "   ...  环 比降  若涨 政府  藏药 产业  西藏 做  销售 降  降 四成  限购 执行  限购 首日  项目 涉嫌  首日 成交  \n",
       "0  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "1  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "2  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "3  ...     0      1      0     0     0     0      0      0      0      0  \n",
       "4  ...     0      0      1     1     0     0      0      0      0      0  \n",
       "5  ...     0      0      0     0     0     0      0      0      1      0  \n",
       "6  ...     1      0      0     0     0     0      0      1      0      1  \n",
       "7  ...     0      0      0     0     0     0      1      0      0      0  \n",
       "8  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "9  ...     0      0      0     0     1     1      0      0      0      0  \n",
       "\n",
       "[10 rows x 55 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize, ngram_range=(2,2))\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中ngram_range选项给出了一个区间，如果是(1,2)那么就是1元、2元词袋同时存在，而(2,2)代表仅使用2元词袋，(1,1)为默认，即只使用一元词袋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF模型\n",
    "\n",
    "词袋模型简单有效，但是哟一个缺点，即只考虑了词的绝对频率，而没有考虑相对频率。比如有的词天然的出现频率更高，那么在每个文档里面，其重要性应该是更低的。\n",
    "\n",
    "而TF-IDF模型就是在词袋的基础上修正这一点。TF-IDF模型是两个度量的乘积：$$TFIDF=TF\\times IDF$$其中TF即词频，而IDF为逆文档频率，IDF的定义为：$$IDF\\left(w\\right)=1+\\ln\\left(\\frac{N}{1+df\\left(w\\right)}\\right)$$其中$N$为文档总数量，而$df\\left(w\\right)$为包含单词$w$的文档个数。\n",
    "\n",
    "可以看到根据上面的定义，一个单词$w$如果出现的文档越多，那么其$IDF\\left(w\\right)$值就越小，或者说权重就越小。\n",
    "\n",
    "最终，我们将上面词袋中每个文档每个词的词频（$TF_i\\left(w\\right),i=1,...,N$）乘以权重就得到了TF-IDF值：$$TFIDF_i\\left(w\\right)=TF_i\\left(w\\right)\\times IDF\\left(w\\right),i=1,...,N$$\n",
    "\n",
    "最后，对每个文档，还需要将以上得到的向量进行标准化，一般使用$L2$范数进行标准化（从而每个向量都在$M$维单位球上，$M$为词的个数）：$$NormalizedTFIDF_i\\left(w\\right)=\\frac{TFIDF_i\\left(w\\right)}{\\sqrt{\\sum_{k=1}^{M}\\left[TFIDF_i\\left(w\\right)\\right]^2}},i=1,...,N$$\n",
    "\n",
    "比如，对于之前计算的词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 59 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以计算「上海」这个词在两个文档中出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+2\\right)=2.20397$，而「做」这个词只有一个文档出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+1\\right)=2.60944$。\n",
    "\n",
    "以上计算略显复杂，不过Scikit-Learn中也给出了方便的计算函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.364296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428537</td>\n",
       "      <td>0.428537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         上海   上调        不探       九条        产业       人民币      价格指数        倾斜  \\\n",
       "0  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.0  0.000000  0.00000  0.000000  0.334845  0.000000  0.000000   \n",
       "2  0.000000  0.5  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.000000  0.0  0.000000  0.00000  0.288675  0.000000  0.000000  0.288675   \n",
       "5  0.000000  0.0  0.359846  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.317517  0.0  0.000000  0.37351  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.0  0.000000  0.00000  0.000000  0.457985  0.000000  0.000000   \n",
       "9  0.364296  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         做  准备金率  ...        藏药        西藏        试点        调控        销售  \\\n",
       "0  0.00000   0.0  ...  0.000000  0.000000  0.460158  0.000000  0.000000   \n",
       "1  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.00000   0.5  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.57735   0.0  ...  0.288675  0.288675  0.000000  0.000000  0.000000   \n",
       "5  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.428537   \n",
       "\n",
       "          降        限购        项目        首日    高  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.5  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "5  0.000000  0.000000  0.359846  0.000000  0.0  \n",
       "6  0.000000  0.305902  0.000000  0.359846  0.0  \n",
       "7  0.000000  0.317517  0.000000  0.000000  0.0  \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "9  0.428537  0.000000  0.000000  0.000000  0.0  \n",
       "\n",
       "[10 rows x 59 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(norm='l2').fit(bag_words) ##使用L2范数，并fit模型（如计算IDF等）\n",
    "tfidf_words=tfidf_transformer.transform(bag_words) ##变换数据\n",
    "tfidf_words_df=pd.DataFrame.sparse.from_spmatrix(tfidf_words, columns=words_names)\n",
    "tfidf_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Scikit-Learn中词袋、TF-IDF计算的具体文档，可以查看：https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更高级的方法：词嵌入\n",
    "\n",
    "上面的词袋模型和TF-IDF模型都非常直观，但是有一个缺点，就是维数非常高。我们仅仅使用了10条新闻标题，就得到了59列特征，虽然在存储和计算上我们可以使用稀疏矩阵，但是在分析中，维数太高的模型分析起来总是比较困难的。而**词嵌入**（**word embedding**）的出现很大程度上缓解了这个问题。\n",
    "\n",
    "所谓词嵌入，实际上是把高维空间的向量向低维空间映射的过程。一个经典的算法是谷歌提出的word2vec算法，关于该算法的原理我们再次不再赘述，我们这里主要通过例子来展示如何使用该方法。\n",
    "\n",
    "Python中可以使用Gensim包实现word2vec算法，在使用之前需要先安装：\n",
    "\n",
    "```shell\n",
    "sudo pip3 install gensim\n",
    "```\n",
    "\n",
    "该算法需要提供如下几个信息：\n",
    "\n",
    "* 语料库，对于中文可以提交已经分好词的语料库\n",
    "* window，窗宽，上下文可以联系起来的单词个数\n",
    "* size，输出的词向量的维度，几十到几千都可以\n",
    "* min_count，只有当某个词出现次数大于该数值时才会被加入到模型中。\n",
    "\n",
    "比如，使用以上新闻标题数据，可以训练如下模型：\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "\n",
    "CORPUS=map(tokenize,RAW1['title'])\n",
    "w2v_model=gensim.models.Word2Vec(CORPUS, window=5, size=10, min_count=5)\n",
    "w2v_model.save('word2vec')##后面节省时间，先把模型保存下来\n",
    "```\n",
    "\n",
    "当然在现实中，有的时候我们也会使用其他人已经预训练的模型。其实在本例中，只使用标题信息训练出的模型精度并不好（比如标题中使用了沪就不会使用上海，因而很难侦测到这种相关性）。\n",
    "\n",
    "如果需要导入预训练的模型，可以直接使用load函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec.load('word2vec')##节省时间，直接导入预训练的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型的具体语法和解释可以查看：https://radimrehurek.com/gensim/models/word2vec.html#module-gensim.models.word2vec\n",
    "\n",
    "有了模型后，我们可以查看每个词对应的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海：\n",
      " [ 0.00492577 -0.046063   -0.02624635  0.02140632 -0.01080333 -0.04793677\n",
      "  0.01134323  0.0151656  -0.01423356  0.02468662]\n",
      "人民币：\n",
      " [-0.03262644  0.00435081  0.01512198 -0.01207902  0.00991976 -0.01236463\n",
      "  0.00525499 -0.04884567  0.03808215 -0.04067073]\n",
      "准备金率：\n",
      " [-0.0061835   0.02351753 -0.01029349  0.02901197 -0.00508447 -0.01825697\n",
      " -0.02109468  0.04761891  0.03359799  0.03474792]\n",
      "价格指数：\n",
      " [-0.0219837  -0.0362713   0.03335079  0.03211081  0.00361599 -0.00924393\n",
      "  0.01231467  0.02797877 -0.0257039   0.01535985]\n",
      "倾斜：\n",
      " [ 0.04428351 -0.02115675  0.03319819  0.04183628 -0.00683692  0.02698661\n",
      "  0.01550904  0.04639012 -0.03696239 -0.03606629]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"上海：\\n\",w2v_model.wv['上海'])\n",
    "print(\"人民币：\\n\",w2v_model.wv['人民币'])\n",
    "print(\"准备金率：\\n\",w2v_model.wv['准备金率'])\n",
    "print(\"价格指数：\\n\",w2v_model.wv['价格指数'])\n",
    "print(\"倾斜：\\n\",w2v_model.wv['倾斜'])\n",
    "w2v_model.wv['倾斜'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以查看跟某些词最相关的词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('投资总额', 0.9196069240570068),\n",
       " ('电器', 0.9121246933937073),\n",
       " ('三亚', 0.9077684283256531),\n",
       " ('深圳机场', 0.8988783955574036),\n",
       " ('希拉里', 0.896324872970581),\n",
       " ('商战', 0.8922805786132812),\n",
       " ('贾庆林', 0.8788282871246338),\n",
       " ('资讯网', 0.8780584335327148),\n",
       " ('批评', 0.8765662908554077),\n",
       " ('腹地', 0.8731513619422913)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['上海'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者跟某个词的向量的相反数最相关的（与$-1\\times$上海 最相关的）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('看齐', 0.9438819885253906),\n",
       " ('spurs', 0.9065861105918884),\n",
       " ('跌势', 0.9038195013999939),\n",
       " ('指数化', 0.8967482447624207),\n",
       " ('虎头蛇尾', 0.894442617893219),\n",
       " ('华微电子', 0.8899738788604736),\n",
       " ('大方', 0.8865758180618286),\n",
       " ('林区', 0.8781821727752686),\n",
       " ('黎瑞刚', 0.8715794682502747),\n",
       " ('财经', 0.8647944927215576)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(negative=['上海'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者查看：上海+杭州-北京=？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('降至', 0.9135311841964722),\n",
       " ('林凤', 0.9026652574539185),\n",
       " ('行业动态', 0.8978720903396606),\n",
       " ('慎行', 0.8964062333106995),\n",
       " ('涨幅', 0.8903248310089111),\n",
       " ('growth', 0.8902656435966492),\n",
       " ('防爆', 0.8879079222679138),\n",
       " ('携', 0.8809096813201904),\n",
       " ('首倡', 0.880886435508728),\n",
       " ('代收', 0.8773815035820007)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['上海','杭州'],negative=['北京'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，直接看两个词的相似度（词向量的相关系数）也是可以的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海,北京的相似度= -0.1084\n",
      "上海,日本的相似度= 0.2207\n",
      "北京,日本的相似度= 0.0350\n",
      "上海,股市的相似度= -0.0933\n",
      "股市,北京的相似度= -0.3320\n"
     ]
    }
   ],
   "source": [
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"北京\",w2v_model.wv.similarity(\"上海\",\"北京\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"日本\",w2v_model.wv.similarity(\"上海\",\"日本\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"北京\",\"日本\",w2v_model.wv.similarity(\"医药\",\"医疗\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"股市\",w2v_model.wv.similarity(\"上海\",\"股市\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"股市\",\"北京\",w2v_model.wv.similarity(\"股市\",\"北京\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，以上算法仅仅计算了每个词的向量，我们数据中的却是文档，因而需要将词向量加总成文档向量。为此，我们可以通过平均的方式求文档的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '地州', '探矿权', '年', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['人民币', '升值', '容忍度', '提高'],\n",
       " ['年', '上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>-0.006618</td>\n",
       "      <td>-0.018949</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.004081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004175</td>\n",
       "      <td>-0.013631</td>\n",
       "      <td>-0.008919</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.010543</td>\n",
       "      <td>-0.004379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>-0.003668</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.014686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011408</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>-0.012170</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>0.015262</td>\n",
       "      <td>-0.008600</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>-0.003283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008140</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.007011</td>\n",
       "      <td>-0.004963</td>\n",
       "      <td>-0.003494</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.014499</td>\n",
       "      <td>-0.010750</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.012101</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>-0.016203</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.016470</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>-0.002090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016397</td>\n",
       "      <td>-0.008823</td>\n",
       "      <td>-0.002454</td>\n",
       "      <td>-0.000952</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>-0.007105</td>\n",
       "      <td>-0.008945</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>-0.007003</td>\n",
       "      <td>-0.007932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001437</td>\n",
       "      <td>-0.014051</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>-0.008594</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>-0.017864</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>-0.023492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.013095</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>-0.009361</td>\n",
       "      <td>-0.010715</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>-0.004939</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>-0.007145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.003951  0.008573 -0.007845  0.029153 -0.001636  0.007078 -0.006618   \n",
       "1  0.004175 -0.013631 -0.008919  0.004198  0.008313 -0.004474  0.003221   \n",
       "2 -0.000862  0.016531 -0.003668  0.012365  0.009207  0.003115 -0.010397   \n",
       "3 -0.011408  0.002533 -0.012170  0.010743  0.008979 -0.002051  0.015262   \n",
       "4 -0.008140  0.014732  0.000375 -0.007011 -0.004963 -0.003494  0.012276   \n",
       "5 -0.012101  0.021007  0.005489 -0.007670 -0.002065 -0.016203  0.018926   \n",
       "6  0.001002  0.004078  0.013386  0.010451  0.016470  0.012947 -0.008411   \n",
       "7  0.016397 -0.008823 -0.002454 -0.000952 -0.001927 -0.007105 -0.008945   \n",
       "8  0.001437 -0.014051  0.004978 -0.008594  0.013979  0.004755  0.006658   \n",
       "9 -0.013095  0.000267  0.010342  0.003259 -0.009361 -0.010715  0.008522   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.018949  0.005949  0.004081  \n",
       "1  0.008576  0.010543 -0.004379  \n",
       "2  0.010822  0.019011  0.014686  \n",
       "3 -0.008600 -0.025539 -0.003283  \n",
       "4  0.014499 -0.010750  0.007832  \n",
       "5  0.011922  0.008909  0.001513  \n",
       "6  0.006189  0.006725 -0.002090  \n",
       "7  0.003442 -0.007003 -0.007932  \n",
       "8 -0.017864  0.008969 -0.023492  \n",
       "9 -0.004939  0.000986 -0.007145  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 知识提要：闭包\n",
    "def mean_vector(model):\n",
    "    def mean_vector_compute(sentence):\n",
    "        n_w=0\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv+=model.wv[w]\n",
    "                except:\n",
    "                    mv=model.wv[w].copy()\n",
    "                n_w+=1\n",
    "        mv/=n_w\n",
    "        return mv\n",
    "    return mean_vector_compute\n",
    "\n",
    "mv_compute=mean_vector(w2v_model)\n",
    "mean_vecs=map(mv_compute,tokenized_data)\n",
    "mean_vecs=pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者我们可以使用TF-IDF进行加权："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003323</td>\n",
       "      <td>0.455663</td>\n",
       "      <td>-0.145274</td>\n",
       "      <td>0.511706</td>\n",
       "      <td>-0.411980</td>\n",
       "      <td>0.277509</td>\n",
       "      <td>-0.451040</td>\n",
       "      <td>-0.200881</td>\n",
       "      <td>0.137241</td>\n",
       "      <td>0.007013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058696</td>\n",
       "      <td>-0.152923</td>\n",
       "      <td>0.042728</td>\n",
       "      <td>0.148408</td>\n",
       "      <td>-0.382521</td>\n",
       "      <td>-0.363477</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>0.391509</td>\n",
       "      <td>-0.105350</td>\n",
       "      <td>0.468349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032507</td>\n",
       "      <td>0.296106</td>\n",
       "      <td>0.292710</td>\n",
       "      <td>0.373906</td>\n",
       "      <td>0.506088</td>\n",
       "      <td>0.195853</td>\n",
       "      <td>-0.147902</td>\n",
       "      <td>0.536458</td>\n",
       "      <td>-0.025744</td>\n",
       "      <td>0.284555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008813</td>\n",
       "      <td>-0.227956</td>\n",
       "      <td>-0.337611</td>\n",
       "      <td>0.432197</td>\n",
       "      <td>0.179107</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.391801</td>\n",
       "      <td>-0.216528</td>\n",
       "      <td>-0.400391</td>\n",
       "      <td>-0.409501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.527672</td>\n",
       "      <td>0.213440</td>\n",
       "      <td>0.194276</td>\n",
       "      <td>-0.427917</td>\n",
       "      <td>-0.219428</td>\n",
       "      <td>-0.362186</td>\n",
       "      <td>0.436368</td>\n",
       "      <td>0.254204</td>\n",
       "      <td>0.019597</td>\n",
       "      <td>-0.142837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.533154</td>\n",
       "      <td>0.535198</td>\n",
       "      <td>0.334695</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.142925</td>\n",
       "      <td>0.181173</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.424654</td>\n",
       "      <td>0.130062</td>\n",
       "      <td>-0.203659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.194803</td>\n",
       "      <td>0.105618</td>\n",
       "      <td>0.175776</td>\n",
       "      <td>0.282755</td>\n",
       "      <td>-0.258024</td>\n",
       "      <td>0.207522</td>\n",
       "      <td>0.470764</td>\n",
       "      <td>0.418189</td>\n",
       "      <td>0.497161</td>\n",
       "      <td>-0.294512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.096772</td>\n",
       "      <td>-0.331754</td>\n",
       "      <td>0.368706</td>\n",
       "      <td>-0.410122</td>\n",
       "      <td>-0.104979</td>\n",
       "      <td>0.164564</td>\n",
       "      <td>0.274626</td>\n",
       "      <td>0.326587</td>\n",
       "      <td>-0.515625</td>\n",
       "      <td>-0.300634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.040217</td>\n",
       "      <td>-0.294157</td>\n",
       "      <td>-0.089638</td>\n",
       "      <td>0.480345</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>0.522841</td>\n",
       "      <td>-0.302417</td>\n",
       "      <td>-0.355450</td>\n",
       "      <td>-0.014571</td>\n",
       "      <td>-0.383468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.181891</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>0.411060</td>\n",
       "      <td>0.049498</td>\n",
       "      <td>0.344841</td>\n",
       "      <td>-0.221050</td>\n",
       "      <td>0.493059</td>\n",
       "      <td>-0.295946</td>\n",
       "      <td>-0.482207</td>\n",
       "      <td>-0.246795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.003323  0.455663 -0.145274  0.511706 -0.411980  0.277509 -0.451040   \n",
       "1  0.058696 -0.152923  0.042728  0.148408 -0.382521 -0.363477  0.535865   \n",
       "2  0.032507  0.296106  0.292710  0.373906  0.506088  0.195853 -0.147902   \n",
       "3  0.008813 -0.227956 -0.337611  0.432197  0.179107  0.294463  0.391801   \n",
       "4  0.527672  0.213440  0.194276 -0.427917 -0.219428 -0.362186  0.436368   \n",
       "5  0.533154  0.535198  0.334695  0.158385  0.142925  0.181173  0.015071   \n",
       "6 -0.194803  0.105618  0.175776  0.282755 -0.258024  0.207522  0.470764   \n",
       "7  0.096772 -0.331754  0.368706 -0.410122 -0.104979  0.164564  0.274626   \n",
       "8 -0.040217 -0.294157 -0.089638  0.480345 -0.186183  0.522841 -0.302417   \n",
       "9  0.181891  0.059960  0.411060  0.049498  0.344841 -0.221050  0.493059   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.200881  0.137241  0.007013  \n",
       "1  0.391509 -0.105350  0.468349  \n",
       "2  0.536458 -0.025744  0.284555  \n",
       "3 -0.216528 -0.400391 -0.409501  \n",
       "4  0.254204  0.019597 -0.142837  \n",
       "5 -0.424654  0.130062 -0.203659  \n",
       "6  0.418189  0.497161 -0.294512  \n",
       "7  0.326587 -0.515625 -0.300634  \n",
       "8 -0.355450 -0.014571 -0.383468  \n",
       "9 -0.295946 -0.482207 -0.246795  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_vector_tfidf_weight(model):\n",
    "    def mean_vector_compute(sentence,tfidf):\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv+=model.wv[w]*tfidf[w][0]\n",
    "                except:\n",
    "                    mv=model.wv[w].copy()\n",
    "        ## L2规范化\n",
    "        import numpy as np\n",
    "        mv/=np.linalg.norm(mv)\n",
    "        return mv\n",
    "    return mean_vector_compute\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "##首先计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "##计算TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(norm='l2').fit(bag_words)\n",
    "tfidf_words=tfidf_transformer.transform(bag_words)\n",
    "tfidf_words_df=pd.DataFrame.sparse.from_spmatrix(tfidf_words, columns=words_names)\n",
    "##带入模型\n",
    "tfidf_weight_mean_vec=mean_vector_tfidf_weight(w2v_model)\n",
    "mean_vecs=[]\n",
    "for i,sentence in enumerate(tokenized_data):\n",
    "    mean_vecs.append(tfidf_weight_mean_vec(sentence,tfidf_words_df.iloc[i,:]))\n",
    "mean_vecs=pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新进展\n",
    "\n",
    "以上介绍的特征提取技术都是非常规范化的提取技术，然而也有很大漏洞。比如「大败」这个词，如果是「巴塞罗那大败皇家马德里」，意思是巴塞罗那赢了，但是如果是「巴塞罗那大败」，那巴塞罗那输了。同理还有更显而易见的，「bank」这个词即是「银行」的意思，也是「河岸」的意思。但是在我们上面的处理中，并没有能够区分同一个词的这些差别。\n",
    "\n",
    "目前在文本挖掘领域，Google的BERT正发展的如火如荼，BERT通过一个大型的深度学习网络，将词语放在句子中进行理解，可以很大程度上克服以上问题。目前也有了包括中文在内的预训练模型，以及TensorFlow和PyTorch等主流框架的实现。\n",
    "\n",
    "最后还是要提示的是，对于非专业机器学习使用者，模型是相对固定的，特征提取是非常关键的。以上介绍的方法并不是唯一的方法，比如我们完全可以根据主观理解，通过一系列的正则表达式定义出很多模式，放在特征中，这些都是特征提取的重要方式。实际应用中需要根据应用背景灵活使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本距离与相似度\n",
    "\n",
    "距离和相似度在文本的概念上都是文本之间某种相似程度的度量，只不过距离（distance）通常用于比较短小的词汇、句子上的差异性有多大，而相似度则主要针对更长的文档等。\n",
    "\n",
    "## 编辑距离\n",
    "\n",
    "经过适当的向量化之后，我们可以轻松地使用向量进行向量空间的任何距离操作，距离越小代表两个字符串之间越相似。在这里我们额外介绍一种常用的距离，即基于编辑距离的Levenshtein距离，该距离度量了从一个字符串str1需要经过多少步的编辑（替换一个字符、插入一个字符、删除一个字符）才能变成另外一个字符串str2。这个步数的计算可以通过动态规划（dynamic programming）来完成。\n",
    "\n",
    "Python中可以安装Levenshtein包：\n",
    "```shell\n",
    "sudo pip3 install python-Levenshtein\n",
    "```\n",
    "\n",
    "计算Levenshtein距离："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "Levenshtein.distance('色即是空，空即是色','色不异空，空不异色')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上给出了最小修改次数，不过最好是将其规范化：$$ratio=\\frac{len\\left(str1\\right)+len\\left(str2\\right)-distance}{len\\left(str1\\right)+len\\left(str2\\right)}$$可以使用Levenstein.ratio()计算该比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "Levenshtein.ratio('色即是空，空即是色','色不异空，空不异色')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编辑距离有很多用处，比如比照不同版本、侦测细微的字符串差异（比如可能存在的地址输入差异「上海松江文汇路」和「上海市松江区文汇路」）、检查抄袭等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "xj_jmls=\"观世音菩萨，行深般若波罗蜜时，照见五阴空，度一切苦厄。舍利弗，色空故，无恼坏相，受空故，无受相，想空故，无知相，行空故，无作相，识空故，无觉相。何以故？舍利弗，非色异空，非空异色，色即是空，空即是色，受想行识，亦复如是。\"\n",
    "xj_xz=\"观自在菩萨，行深般若波罗蜜多时，照见五蕴皆空，度一切苦厄。舍利子，色不异空，空不异色，色即是空，空即是色，受想行识亦复如是。\"\n",
    "Levenshtein.ratio(xj_jmls,xj_xz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本相似度\n",
    "\n",
    "不管是使用词袋模型，还是TF-IDF模型，或者使用词嵌入方法通过平均、加权平均的方式，都可以讲一个文本向量化。\n",
    "\n",
    "将文本向量化之后，计算文本之间的相似度就非常简单了：只要将两个文本的向量之间的相似度计算出来即可，常用的度量是余弦相似度（即两个向量的夹角）：$$cs\\left(u,v\\right)=\\frac{u\\cdot v}{\\left\\Vert u\\right\\Vert \\cdot \\left\\Vert v\\right\\Vert }$$其中分子为内积，分母上位L2范数的乘积。我们可以使用NumPy很快的计算出该值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cos_similarity=lambda u,v: np.inner(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如如果我们使用词袋模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14433756729740646"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(bag_words_df.iloc[7,:],bag_words_df.iloc[9,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也可以找出于某一个标题最为相似的，内存限制只算前50000个（如果要算所有的可以分开来算）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沪争取增值税扩围改革试点\n",
      "增值税扩围和资源税改革今年有望取得突破\n"
     ]
    }
   ],
   "source": [
    "##计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(RAW1['title'][:50000])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "##计算相关系数（自己用循环试一下，奇慢无比，如果不向量化计算，估计要跑的时间按天算）\n",
    "ip=np.array(np.dot(bag_words_df,bag_words_df.iloc[0,:])) ## 这里用到了广播\n",
    "norm1=np.array(np.linalg.norm(bag_words_df, axis=1)) ## 第一行的norm都一样，所以不用除\n",
    "corr=ip/norm1\n",
    "corr=corr[1:]\n",
    "##找最大值\n",
    "argi=np.argmax(corr)\n",
    "print(RAW1['title'].iloc[0])\n",
    "print(RAW1['title'].iloc[argi+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在其他向量化方法下同理，在此不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类\n",
    "\n",
    "同样，建立在以上特征提取的基础上，我们已经能够将文本数据转化为向量，那么接下来，任何的机器学习算法也都可以应用到这些词向量上面。\n",
    "\n",
    "但是，由于文本数据的高度复杂性和非线性性、高维性，传统的Logistic回归等方法肯定不再适用，此时支持向量机、贝叶斯方法、决策树、随机森林都可以试一下。\n",
    "\n",
    "当然，模型是固定的，模型的好坏很大程度上取决于特征提取的方式以及参数设定。\n",
    "\n",
    "\n",
    "我们下面给一个例子作为示例，即使用每天的新闻数据预测沪深300指数的涨跌。\n",
    "\n",
    "其中沪深300的数据和处理过程我们放在下面，为了不造成服务器压力，我们把数据已经存储下来：\n",
    "\n",
    "```python\n",
    "## 导入股票数据\n",
    "import akshare as ak\n",
    "import pandas as pd\n",
    "stock_data=ak.stock_zh_index_daily(symbol=\"sh000300\")\n",
    "## 由于pandas的时间滞后会导致缺失值，手动产生滞后\n",
    "stock_data=stock_data.reset_index()\n",
    "last_trade_day=stock_data['close']\n",
    "last_trade_day=last_trade_day.reset_index()\n",
    "last_trade_day=last_trade_day.drop('index',axis=1)\n",
    "stock_data=stock_data.drop(0)\n",
    "stock_data=stock_data.reset_index()\n",
    "stock_data['last_close']=last_trade_day\n",
    "stock_data['tag']=stock_data['last_close']<stock_data['close']\n",
    "## 设定时间\n",
    "stock_data['date']=pd.to_datetime(stock_data['date'])\n",
    "stock_data.to_csv('csv/stock_index.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>last_close</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-07</td>\n",
       "      <td>1302.084</td>\n",
       "      <td>1302.084</td>\n",
       "      <td>1302.084</td>\n",
       "      <td>1302.084</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1316.455</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>1292.714</td>\n",
       "      <td>1292.714</td>\n",
       "      <td>1292.714</td>\n",
       "      <td>1292.714</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1302.084</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-01-09</td>\n",
       "      <td>1272.645</td>\n",
       "      <td>1272.645</td>\n",
       "      <td>1272.645</td>\n",
       "      <td>1272.645</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1292.714</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-10</td>\n",
       "      <td>1281.261</td>\n",
       "      <td>1281.261</td>\n",
       "      <td>1281.261</td>\n",
       "      <td>1281.261</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1272.645</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-01-11</td>\n",
       "      <td>1249.814</td>\n",
       "      <td>1249.814</td>\n",
       "      <td>1249.814</td>\n",
       "      <td>1249.814</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1281.261</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>3542.684</td>\n",
       "      <td>3585.801</td>\n",
       "      <td>3523.788</td>\n",
       "      <td>3530.306</td>\n",
       "      <td>1.371088e+10</td>\n",
       "      <td>3653.224</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>3598.655</td>\n",
       "      <td>3627.759</td>\n",
       "      <td>3545.452</td>\n",
       "      <td>3625.115</td>\n",
       "      <td>1.431030e+10</td>\n",
       "      <td>3530.306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>3711.475</td>\n",
       "      <td>3732.654</td>\n",
       "      <td>3685.994</td>\n",
       "      <td>3722.518</td>\n",
       "      <td>1.534960e+10</td>\n",
       "      <td>3625.115</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>3692.608</td>\n",
       "      <td>3736.254</td>\n",
       "      <td>3681.267</td>\n",
       "      <td>3698.047</td>\n",
       "      <td>1.123107e+10</td>\n",
       "      <td>3722.518</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>3746.391</td>\n",
       "      <td>3758.783</td>\n",
       "      <td>3709.921</td>\n",
       "      <td>3710.061</td>\n",
       "      <td>1.240610e+10</td>\n",
       "      <td>3698.047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4422 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      open      high       low     close        volume  \\\n",
       "0    2002-01-07  1302.084  1302.084  1302.084  1302.084  0.000000e+00   \n",
       "1    2002-01-08  1292.714  1292.714  1292.714  1292.714  0.000000e+00   \n",
       "2    2002-01-09  1272.645  1272.645  1272.645  1272.645  0.000000e+00   \n",
       "3    2002-01-10  1281.261  1281.261  1281.261  1281.261  0.000000e+00   \n",
       "4    2002-01-11  1249.814  1249.814  1249.814  1249.814  0.000000e+00   \n",
       "...         ...       ...       ...       ...       ...           ...   \n",
       "4417 2020-03-23  3542.684  3585.801  3523.788  3530.306  1.371088e+10   \n",
       "4418 2020-03-24  3598.655  3627.759  3545.452  3625.115  1.431030e+10   \n",
       "4419 2020-03-25  3711.475  3732.654  3685.994  3722.518  1.534960e+10   \n",
       "4420 2020-03-26  3692.608  3736.254  3681.267  3698.047  1.123107e+10   \n",
       "4421 2020-03-27  3746.391  3758.783  3709.921  3710.061  1.240610e+10   \n",
       "\n",
       "      last_close    tag  \n",
       "0       1316.455  False  \n",
       "1       1302.084  False  \n",
       "2       1292.714  False  \n",
       "3       1272.645   True  \n",
       "4       1281.261  False  \n",
       "...          ...    ...  \n",
       "4417    3653.224  False  \n",
       "4418    3530.306   True  \n",
       "4419    3625.115   True  \n",
       "4420    3722.518  False  \n",
       "4421    3698.047   True  \n",
       "\n",
       "[4422 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "stock_data=pd.read_csv('csv/stock_index.csv')\n",
    "stock_data['date']=pd.to_datetime(stock_data['date'])\n",
    "stock_data=stock_data.drop(['Unnamed: 0','index'], axis=1)\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型......\n"
     ]
    }
   ],
   "source": [
    "## 导入数据\n",
    "RAW=pd.read_csv(\"csv/stocknews1.csv\")\n",
    "RAW['date']=pd.to_datetime(RAW['date'])\n",
    "jianbao=RAW['title'].str.match(r'.+\\d{4}年\\d{2}月\\d{2}日.+简报')\n",
    "RAW=RAW.iloc[list(~jianbao),:]\n",
    "##合并数据\n",
    "data=pd.merge(RAW,stock_data[['date','tag']], left_on='date', right_on='date')\n",
    "y=data['tag']\n",
    "##接下来处理文本数据，我们使用简单的词袋模型作为预测特征\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def not_digit(w):\n",
    "    w=w.replace(',','')\n",
    "    if re.match(r'\\d+',w)!=None or re.match(r'\\d%',w)!=None or re.match(r'\\d*\\.\\d+',w)!=None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if ((w not in stoplist) and not_digit(w) and len(w.strip())>0)]\n",
    "    return cut_w\n",
    "\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "#words_names=count_vect.get_feature_names()\n",
    "#bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "\n",
    "##训练模型\n",
    "print(\"开始训练模型......\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##1000颗树，特征数量为总数量的log2\n",
    "rfc=RandomForestClassifier(n_estimators=1000, max_samples=0.5, max_features='log2')\n",
    "rfc.fit(bag_words,y)\n",
    "## 预测及概率\n",
    "data['prob']=rfc.predict_proba(X)[:,1]\n",
    "data['pred']=rfc.predict(X)\n",
    "## 计算指标\n",
    "TP=np.sum(data['tag'] & data['pred'])\n",
    "TN=np.sum((~data['tag'] & (~data['pred'])))\n",
    "FP=np.sum((~data['tag'] & (data['pred'])))\n",
    "FN=np.sum((data['tag']) & (~data['pred']))\n",
    "\n",
    "print(\"TP=\",TP)\n",
    "print(\"TN=\",TN)\n",
    "print(\"FP=\",FP)\n",
    "print(\"FN=\",FN)\n",
    "print(\"查全率=敏感性=\",TP/(TP+FN))\n",
    "print(\"查准率=\",TP/(TP+FP))\n",
    "print(\"特异性=\",TN/(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(data['tag'], data['prob'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.plot(fpr,tpr,label='ROC, AUC=%.2f' % roc_auc)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

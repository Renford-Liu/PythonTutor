{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析的基本概念\n",
    "\n",
    "跟我们之前介绍的分析不同，文本数据是典型的非结构化数据，而且本身具有非常复杂的结构，相对之前的数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词\n",
    "\n",
    "中文文本分析与英文的文本分析一个最重要的区别在于，英文使用空格分割每个单词，但是中文没有分割单词的概念。比如在之前的词频统计中，我们可以简单的使用\n",
    "```python\n",
    "str.split(' ')\n",
    "```\n",
    "\n",
    "把所有的单词分解为一个列表，但是对于中文而言，显然这是行不通的。\n",
    "\n",
    "为了克服这个问题，分词就应运而生了。结合字典和算法，分词软件可以帮助我们将中文的文章、句子分解为一个个的中文单词。\n",
    "\n",
    "目前已经有很多成熟的分词工具，比如中科院的NLPIR汉语分词系统、结巴分词以及腾讯、阿里、百度的分词系统等等。在这里我们以开源的结巴分词( https://github.com/fxsjy/jieba )为例，介绍分词工具的用法。\n",
    "\n",
    "为了使用结巴分词，首先需要安装。在terminal中输入：\n",
    "```shell\n",
    "pip install jieba\n",
    "```\n",
    "\n",
    "就可以进行安装了。安装好之后，可以将jieba模块导入到Python程序中，就可以正常使用了：\n",
    "```shell\n",
    "import jieba\n",
    "```\n",
    "\n",
    "比如，最简单的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "line=\"今年以来，我国持续推进减税降费、提高最低工资标准、促进就业，特别是年初开始实施的个人所得税改革以及专项附加扣除方案，有效增加了居民可支配收入。与此同时，不断消除居民消费的后顾之忧。消费需求进一步释放，消费市场亮点纷呈\"\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut()函数有三个参数：必须要提供的是需要进行分词的字符串；此外，cut_all参数控制是否采用全模式；HMM参数用来控制是否使用HMM模型。其区别是：\n",
    "\n",
    "* cut_all=True， 代表使用全模式，全模式可以切出混合不同粒度的词\n",
    "* HMM=True，代表使用HMM模型，用于推断字典中没有的词\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '', '', '我国', '持续', '推进', '减税', '降', '费', '', '', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '', '', '促进', '就业', '', '', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '', '', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '', '', '与此', '与此同时', '同时', '', '', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '', '', '消费', '需求', '求进', '进一步', '一步', '释放', '', '', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '', '', '我国', '持续', '推进', '减税', '降', '费', '', '', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '', '', '促进', '就业', '', '', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '', '', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '', '', '与此', '与此同时', '同时', '', '', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '', '', '消费', '需求', '求进', '进一步', '一步', '释放', '', '', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面的分词结果......一言难尽。实际上，任何分词算法都不可避免的不能跟上时代的潮流，特别是网络时代，新的词语层出不穷，而在一些专业领域中，一些专有名词往往普通词典无法完全覆盖。比如“减费降税”、“可支配收入”这些专有名词，都没有被正确分出来。\n",
    "\n",
    "为了克服这个问题，往往需要用户自己添加字典。比如，我们可以把“减费降税”、“可支配收入”这些名词放在一个文本文件中，每个新词写成一行，然后使用load_userdict()函数给定这个文件，就可以添加自己的新词列表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('user_dict.txt','wt') as f:\n",
    "    f.write(\"减税降费\\n可支配收入\\n最低工资\\n最低工资标准\")\n",
    "jieba.load_userdict('user_dict.txt')\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入了用户字典后，新的分词更加准确了。在实际应用的时候，无论使用什么分词工具，用户字典的构建往往是非常关键的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 停用词\n",
    "\n",
    "分词之后，在进一步进行处理之前，消除停用词往往是非常关键的。比如在上面的分词结果中，“的”、“是”、“与此同时”等，含义并不是非常明显，对其分析的意义不大，留着这些词只会空占内存，并且可能对分析结果产生巨大影响。一个常用的做法是，使用一个停用词列表，分词结束之后，把停用词列表中的词全都剔除出去。\n",
    "\n",
    "比如，在文本文档“chinese_cutting/stopword.txt”中，我们列举除了一些常用的停用词，我们可以使用如下方法消除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '我国', '持续', '推进', '减税降费', '提高', '最低工资标准', '就业', '特别', '年初', '实施', '个人所得税', '改革', '专项', '附加', '扣除', '方案', '增加', '居民', '可支配收入', '消除', '居民消费', '后顾之忧', '消费', '需求', '进一步', '释放', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "wlist=jieba.cut(line)\n",
    "wlist=[w for w in wlist if w not in stoplist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果就清晰很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个例子：中文词频统计\n",
    "\n",
    "以下实现了对《越女剑》的词频统计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      范蠡 : 118\n",
      "       道 : 113\n",
      "      剑士 : 105\n",
      "      青衣 : 47\n",
      "      阿青 : 47\n",
      "      勾践 : 44\n",
      "      锦衫 : 35\n",
      "      长剑 : 34\n",
      "      说道 : 31\n",
      "      吴国 : 30\n",
      "      薛烛 : 29\n",
      "       吴 : 26\n",
      "      西施 : 26\n",
      "      少女 : 26\n",
      "     国剑士 : 25\n",
      "       说 : 24\n",
      "     伍子胥 : 24\n",
      "      竹棒 : 24\n",
      "       剑 : 23\n",
      "      文种 : 23\n",
      "      一声 : 22\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "## 停用词\n",
    "with open('Chinese_cutting/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "## 读入小说\n",
    "wordlist=[]\n",
    "with open('Chinese_cutting/越女剑.txt','rt') as f:\n",
    "    for l in f:\n",
    "        line_cut=jieba.cut(l)\n",
    "        line=[w.replace('\\n','').strip() for w in line_cut]\n",
    "        wordlist.extend(line)\n",
    "wordlist=[w for w in wordlist if w not in stoplist]\n",
    "## 统计\n",
    "text_dict=dict()\n",
    "for l in wordlist:\n",
    "    if l not in text_dict:\n",
    "        text_dict[l]=1\n",
    "    else:\n",
    "        text_dict[l]+=1\n",
    "text_freq=[]\n",
    "for k in text_dict:\n",
    "    text_freq.append((k,text_dict[k]))\n",
    "## 排序\n",
    "text_freq.sort(key=lambda x: x[1],reverse=True)\n",
    "max_print=20\n",
    "times=0\n",
    "for t in text_freq:\n",
    "    print(\"%8s : %2d\" % t)\n",
    "    times+=1\n",
    "    if times>max_print:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

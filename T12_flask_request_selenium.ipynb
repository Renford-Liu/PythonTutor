{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫简介\n",
    "\n",
    "爬虫一般指从网站上获得需要的数据，这个过程其实是建造一个网站的逆向过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Flask建立简单网站\n",
    "\n",
    "所谓知己知彼，则百战不殆。既然爬虫主要是从网站上爬取数据，那么只有对建造网站的步骤一清二楚，才能在变幻多端的网站环境下，获取想要的数据。\n",
    "\n",
    "然而实际建立网站的过程是非常繁琐而复杂的，写HTML、CSS仅仅是一个网站的冰山一角，这其中至少涉及到网站架构设计、数据库设计、后台服务设计、前端设计等等，此外根据不同的架构设计，还有很多中间件、缓存数据库等等复杂的设计。\n",
    "\n",
    "不过，对于比较简单的网站，Python本身就有很多成熟的框架可以方便我们快速搭建一个简单的网站。这其中，Flask由于其比较精美的设计架构以及简单的模板等应用，是非常受欢迎的轻量级Web框架。在这里，我们不妨使用Flask搭建一个最简单的网站（因为在Jupyter里面，所以我把run()给注释掉了，如果需要执行，请直接执行html/web.py）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "import random\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    with open(\"example4.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic')\n",
    "def dynamic():\n",
    "    with open(\"dynamic.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic_response')\n",
    "def dynamic_response():\n",
    "    return str(random.random())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行以上命令之前，需要首先使用pip install flask安装Flask。\n",
    "\n",
    "在上面的程序中，@代表修饰器（decorator），是Python编程的一个高级特性，我们暂且不管。我们需要知道的仅仅是，通过@app.route函数，声明了一个路径，该路径即访问接下来定义的页面的路径。\n",
    "\n",
    "在@app.route下方，我们定义了几个函数，这几个函数的作用是返回一个字符串，这些字符串会通过网络传递给访问的浏览器。\n",
    "\n",
    "如果运行html/web.py，会提示：\n",
    ">  Running on http://127.0.0.1:5000/\n",
    "\n",
    "此时，如果在浏览器中输入以上网址，服务器就会执行index()函数，该函数会读取example4.html，并将其返回，从而我们在浏览器上就看到了example4.html。\n",
    "\n",
    "同理，如果访问 http://127.0.0.1:5000/dynamic ，服务器就会执行dynamic()函数，将dynamic.html的内容返回。\n",
    "\n",
    "而如果访问 http://127.0.0.1:5000/dynamic_response ，服务器会执行dynamic_response()函数，该函数生成一个随机数并返回给浏览器。\n",
    "\n",
    "以上就是一个简单的网站。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发送请求\n",
    "\n",
    "知道了服务器端如何处理页面，那么现在的问题是，客户端怎样与服务器端通讯呢？\n",
    "\n",
    "这里就要介绍以下HTTP协议的概念了。\n",
    "\n",
    "HTTP协议是**Hyper Text Transfer Protocol**（**超文本传输协议**）的缩写，一种服务器/客户端范式的传输协议，我们访问网址（URL）时，都是以http:// 开头的，或者https:// 开头，代表的就是使用http协议。\n",
    "\n",
    "一个最简单的传输模型是，客户端通过URL地址向服务器端发送**请求**（**request**），服务器根据所请求的地址、头部信息（headers）做出**响应**（**response**）。\n",
    "\n",
    "这里有几个概念要特别注意：\n",
    "\n",
    "## 网址URL\n",
    "\n",
    "一个常见的网址通常具有如下形式：\n",
    "\n",
    "> http://econpaper.cn:8080/article/article.jsp?id=56987&userid=3455\n",
    "\n",
    "其中：\n",
    "\n",
    "* http:// 代表协议\n",
    "* econpaper.cn 代表域名\n",
    "* :8080 代表端口号，默认为80端口\n",
    "* /article/article.jsp 部分为请求的页面路径\n",
    "* ?id=56987&userid=3455 为需要传递给这个页面的参数\n",
    "\n",
    "## 请求（request）\n",
    "\n",
    "客户端向服务器端发送请求，要按照一定的格式，请求消息由以下三部分组成：\n",
    "\n",
    "* 请求行（request line）\n",
    "* 头部（header）\n",
    "* 请求数据\n",
    "\n",
    "![](pic/request.png)\n",
    "\n",
    "比如，以下是一个典型的请求头部：\n",
    "\n",
    "![](pic/request_headers.png)\n",
    "\n",
    "这些信息传递给服务器后，服务器根据这些信息进行处理。\n",
    "\n",
    "实际上，在我们刚刚运行的Flask中，在客户端可以看到每一次的请求。\n",
    "\n",
    "此外，上面从URL地址中已经看到，?id=56987&userid=3455 为需要传递给这个页面的参数，实际上，这是一个GET的请求方法。为了从客户端向服务器端传数据，以下两种方法是最常用的：\n",
    "\n",
    "* GET：像上面一样，请求的数据明文写在URL上\n",
    "* POST：数据包含在请求体中\n",
    "\n",
    "当然，两种方法也可以结合起来使用。不过最为常用的仍然是GET方法。\n",
    "\n",
    "## 响应（response）\n",
    "\n",
    "有了请求，就会有服务器的响应。同样，响应也有响应头和数据体。在响应头中，最重要的是响应的状态码以及内容类型（html文档或者图片、pdf等），常见的状态码比如：\n",
    "\n",
    "* 200 OK                        客户端请求成功\n",
    "* 400 Bad Request               客户端请求有语法错误，不能被服务器所理解\n",
    "* 401 Unauthorized              请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 \n",
    "* 403 Forbidden                 服务器收到请求，但是拒绝提供服务\n",
    "* 404 Not Found                 请求资源不存在，eg：输入了错误的URL\n",
    "* 500 Internal Server Error     服务器发生不可预期的错误\n",
    "* 503 Server Unavailable        服务器当前不能处理客户端的请求，一段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中发送请求\n",
    "\n",
    "在Python中，有很多工具可以帮助我们向服务器发送请求，包括但不限于：urllib、urllib2、urllib3、httplib2、requests等等等等。出于个人偏好原因，在这里我们以requests为例，介绍如何发送请求。\n",
    "\n",
    "为了使用requests，需要先进行安装：pip install requests\n",
    "\n",
    "使用时直接导入：\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "接着，最简单的请求即GET请求：\n",
    "```python\n",
    "r=requests.get(url)\n",
    "```\n",
    "\n",
    "其中url为需要请求的地址。\n",
    "\n",
    "有时我们在发送请求时可能需要控制发送请求的头部，头部可以使用一个字典表示，比如：\n",
    "```python\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "```\n",
    "\n",
    "以上字典定义了头部的内容，接下来只要在get()函数中加入该头部就可以了：\n",
    "```python\n",
    "r=requests.get(url, headers=hearders)\n",
    "```\n",
    "\n",
    "返回结果：\n",
    "\n",
    "* r.text 可以读取返回的HTML\n",
    "* r.json() 获得Json数据转换后的字典数据\n",
    "* r.encoding 记录了返回数据的字符编码\n",
    "* r.status_code 记录了响应的状态码\n",
    "\n",
    "如果需要发送POST请求，需要首先将请求的数据写成字典形式，在使用urllib包中的urlencode函数将其编码，比如：\n",
    "```python\n",
    "data=dict(name=\"Joe\", comment=\"A test comment\")\n",
    "r=requests.post(url, data=data)\n",
    "```\n",
    "\n",
    "如果需要设置自定义的cookie到服务器，可以使用：\n",
    "```python\n",
    "cookies = dict(cookies_are='working')\n",
    "r=requests.get(url, cookies=cookies)\n",
    "```\n",
    "\n",
    "如果我们希望设置最长的等待时间，可以使用超时选项：\n",
    "```python\n",
    "r=requests.get(url, timeout=10)\n",
    "```\n",
    "\n",
    "即设定等待时间超过10s则放弃连接。\n",
    "\n",
    "最后，如果需要使用代理，可以使用：\n",
    "```python\n",
    "proxies = {\n",
    "  'http': 'http://user@password@10.10.1.10:3128',\n",
    "  'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "r=requests.get(url, proxies=proxies)\n",
    "```\n",
    "\n",
    "比如，使用pm25.in提供的接口爬取pm2.5数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Sorry，您这个小时内的API请求次数用完了，休息一下吧！'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"http://www.pm25.in/api/querys/pm2_5.json?city=shanghai&token=5j1znBVAsnSf5xQyNQyq\"\n",
    "import requests\n",
    "r=requests.get(url)\n",
    "data=r.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数接口都需要用一个token进行身份验证，一般会根据token进行一定的数量限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码实现了从链家上爬去房价数据。值得注意的是，如果没有设定headers，可能会爬不下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['107102052326', '2室2厅', '83.13平米', '新桥', '245万', '新桥/2室2厅/83.13平米/南/精装']\n",
      "['107102235592', '3室2厅', '88.74平米', '泗泾', '302万', '泗泾/3室2厅/88.74平米/南 北/精装']\n",
      "['107102130084', '2室2厅', '89平米', '九亭', '362万', '九亭/2室2厅/89平米/南/精装']\n",
      "['107102210008', '2室2厅', '90.65平米', '九亭', '378.8万', '九亭/2室2厅/90.65平米/南 北/精装']\n",
      "['107101778575', '2室1厅', '70.8平米', '泗泾', '202万', '泗泾/2室1厅/70.8平米/南/简装']\n",
      "['107102269727', '4室2厅', '193.05平米', '松江大学城', '580万', '松江大学城/4室2厅/193.05平米/南 北/简装']\n",
      "['107101202999', '4室2厅', '186平米', '松江新城', '566万', '松江新城/4室2厅/186平米/南/简装']\n",
      "['107102035432', '2室1厅', '62.64平米', '松江老城', '177万', '松江老城/2室1厅/62.64平米/南/简装']\n",
      "['107101355197', '4室2厅', '175.02平米', '莘闵别墅', '685万', '莘闵别墅/4室2厅/175.02平米/南/精装']\n",
      "['107102250821', '3室2厅', '92.08平米', '松江大学城', '365万', '松江大学城/3室2厅/92.08平米/南/精装']\n",
      "['107102283498', '3室2厅', '100.28平米', '九亭', '445万', '九亭/3室2厅/100.28平米/南/简装']\n",
      "['107102299041', '1室1厅', '52.79平米', '泗泾', '155万', '泗泾/1室1厅/52.79平米/南/毛坯']\n",
      "['107101170742', '2室2厅', '90.91平米', '九亭', '353万', '九亭/2室2厅/90.91平米/南/精装']\n",
      "['107102212763', '3室2厅', '93平米', '松江大学城', '385万', '松江大学城/3室2厅/93平米/南/精装']\n",
      "['107102239879', '3室2厅', '115.28平米', '松江老城', '325万', '松江老城/3室2厅/115.28平米/南/精装']\n",
      "['107102204305', '3室2厅', '90.11平米', '松江大学城', '375万', '松江大学城/3室2厅/90.11平米/南/简装']\n",
      "['107102215134', '3室1厅', '83.32平米', '泗泾', '212.9万', '泗泾/3室1厅/83.32平米/南/毛坯']\n",
      "['107102248228', '3室2厅', '130.67平米', '九亭', '445万', '九亭/3室2厅/130.67平米/南/精装']\n",
      "['107102266723', '3室2厅', '115.98平米', '九亭', '550万', '九亭/3室2厅/115.98平米/南/精装']\n",
      "['107102271401', '2室2厅', '95.2平米', '九亭', '380万', '九亭/2室2厅/95.2平米/南 北/精装']\n",
      "['107102275941', '2室2厅', '80.83平米', '松江老城', '276万', '松江老城/2室2厅/80.83平米/南/精装']\n",
      "['107102277147', '2室2厅', '89平米', '泗泾', '350万', '泗泾/2室2厅/89平米/南 北/精装']\n",
      "['107102156808', '3室2厅', '155.91平米', '松江新城', '485万', '松江新城/3室2厅/155.91平米/南/精装']\n",
      "['107100231113', '2室2厅', '88平米', '九亭', '368万', '九亭/2室2厅/88平米/南/毛坯']\n",
      "['107102235923', '2室1厅', '77.71平米', '泗泾', '249万', '泗泾/2室1厅/77.71平米/南/精装']\n",
      "['107102248549', '1室1厅', '56.59平米', '泗泾', '168万', '泗泾/1室1厅/56.59平米/南/精装']\n",
      "['107102290984', '1室2厅', '59.7平米', '松江大学城', '252万', '松江大学城/1室2厅/59.7平米/南/精装']\n",
      "['107100516573', '6室4厅', '349.39平米', '泗泾', '1580万', '泗泾/6室4厅/349.39平米/南/精装']\n",
      "['107102234290', '2室2厅', '71.94平米', '松江大学城', '322万', '松江大学城/2室2厅/71.94平米/南/精装']\n",
      "['107101354141', '2室2厅', '93.11平米', '九亭', '330万', '九亭/2室2厅/93.11平米/南/精装']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "url='https://sh.lianjia.com/ershoufang/songjiang/'\n",
    "r=requests.get(url, headers=headers)\n",
    "html=r.text\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='div',attrs={\"class\":\"item\"})\n",
    "info=[]\n",
    "for it in iterms:\n",
    "    house_id=it.get(\"data-houseid\")\n",
    "    prop=it.find(name='div',attrs={\"class\":\"info\"}).text\n",
    "    shi_ting=re.search(r\"\\d室\\d厅\",prop).group()\n",
    "    area=re.search(r\"\\d*(\\.\\d*)?平米\",prop).group()\n",
    "    location=prop[:prop.find('/')]\n",
    "    price=it.find(name='div',attrs={\"class\":\"price\"}).text\n",
    "    print([house_id,shi_ting,area,location,price,prop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用一个更加综合的例子：百度新闻的爬取："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "### funcs.py\n",
    "#!/usr/bin/python3\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "## write log file.\n",
    "def writeLog(content):\n",
    "    try:\n",
    "        logfile=open('News.log','a')\n",
    "        ltime=time.localtime()\n",
    "        trstr=lambda s:('0'+str(s))[-2:]\n",
    "        logfile.write(str(ltime.tm_year)+'-'+trstr(ltime.tm_mon)+'-'+trstr(ltime.tm_mday)+' '\n",
    "                      +trstr(ltime.tm_hour)+':'+trstr(ltime.tm_min)+':'+trstr(ltime.tm_sec)+'-->'+\n",
    "                      content+'\\n')\n",
    "        logfile.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "## get htmltext\n",
    "def getPage(url,trytimes=10,tot=10):\n",
    "    trytime=1\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "        }\n",
    "    while True:\n",
    "        try:\n",
    "            r=requests.get(url, headers=headers, timeout=tot, stream=False)\n",
    "            htmlText=r.text\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if trytime>=trytimes:\n",
    "                print(e)\n",
    "                writeLog('Error happened in reading '+url+':'+str(e)+'.')\n",
    "                return ''\n",
    "            else:\n",
    "                trytime=trytime+1\n",
    "                time.sleep(10)\n",
    "    return htmlText\n",
    "# get current date\n",
    "def getDate():\n",
    "    ts=time.localtime()\n",
    "    trstr=lambda s:('0'+str(s))[-2:]\n",
    "    return '-'.join((str(ts.tm_year),trstr(ts.tm_mon),trstr(ts.tm_mday)))+' '+':'.join((trstr(ts.tm_hour),trstr(ts.tm_min)))\n",
    "# get the longest content\n",
    "def getLongText(url):\n",
    "    html=getPage(url,trytimes=2)\n",
    "    bs=BeautifulSoup(html,\"html.parser\")\n",
    "    content=\"\"\n",
    "    ## delete all scripts\n",
    "    while bs.script!=None:\n",
    "        bs.script.extract()\n",
    "    while bs.style!=None:\n",
    "        bs.style.extract()\n",
    "    ## get content, method 1\n",
    "    divs=bs.find_all('div')\n",
    "    text=\"\"\n",
    "    minlen=5\n",
    "    for d in divs:\n",
    "        # if d.find_all(\"div\")==[]:\n",
    "        text=d.get_text().strip('\\n\\r ')\n",
    "        if text is not None and text not in content:\n",
    "            if text.count('。')>=1 and len(text)>minlen:\n",
    "                content=content+text\n",
    "    ## get content, method 2\n",
    "    ps=bs.find_all('p')\n",
    "    content2=\"\"\n",
    "    if ps!=[]:\n",
    "        for p in ps:\n",
    "            text=p.get_text()\n",
    "            if text is not None and text not in content2:\n",
    "                if text.count('。')>=1 and len(text)>minlen:\n",
    "                    content2=content2+text\n",
    "    ## compare\n",
    "    if len(content)<=len(content2):\n",
    "        content=content2\n",
    "    ## filtering\n",
    "    content=content.split(\"\\n\")\n",
    "    longtext=\"\"\n",
    "    for c in content:\n",
    "        if len(c)>=10 and (\"。\" in c or \"！\" in c or \"？\" in c):\n",
    "            longtext=longtext+\"\\n\"+c\n",
    "    return longtext.replace(\"百度首页登录个人中心帐号设置意见反馈退出\",'')\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "### News.py\n",
    "#!/usr/bin/python3\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from funs import writeLog\n",
    "from funs import getPage\n",
    "from funs import getDate\n",
    "from funs import getLongText\n",
    "import time\n",
    "import sys\n",
    "## get page\n",
    "def getSubPage(a,longtxt=1):\n",
    "    ## string handler\n",
    "    treatStr=lambda s: s.replace('\\'','\\'\\'')\n",
    "    ## nega words\n",
    "    negaurlwords=('letv','youku','tudou','iqiyi','jfinfo','gmw.cn')\n",
    "    negatitlewords=(\"(图)\",\"（图）\",\"车展\",\"重磅\",\"健康问答\",\"养生\",\"(组图)\",\"（组图）\",\"专治\")\n",
    "    ## url\n",
    "    url=a.get(\"href\")\n",
    "    title=a.get_text().strip(\"\\r\\n \")\n",
    "    if title!=None and url!=None:\n",
    "        if url.find('http://')>=0 and len(title)>4:\n",
    "            ## exist?\n",
    "            sql=\"SELECT count(*) FROM news WHERE url=\\'\"+treatStr(url)+\"\\' LIMIT 1\"\n",
    "            cur.execute(sql)\n",
    "            exist=cur.fetchall()[0][0]\n",
    "            if exist==0:\n",
    "                skip=0\n",
    "                for w in negaurlwords:\n",
    "                    if url.find(w)>=0:\n",
    "                        skip=1\n",
    "                        break\n",
    "                if skip==0:\n",
    "                    for w in negatitlewords:\n",
    "                        if title.find(w)>=0:\n",
    "                            skip=1\n",
    "                            break\n",
    "                if skip==0:\n",
    "                    content=getLongText(url)\n",
    "                    if content!=None and len(content)>=80:\n",
    "                        return (getDate(),title,url,content)\n",
    "    return None\n",
    "## init database\n",
    "conn=sqlite3.connect('News.db')\n",
    "cur=conn.cursor()\n",
    "createTable=(\"CREATE TABLE IF NOT EXISTS news (\"\n",
    "             \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n",
    "             \"date DATE, \"\n",
    "             \"title VARCHAR(500),\"\n",
    "             \"url VARCHAR(800),\"\n",
    "             \"content LONGTEXT);\")\n",
    "conn.execute(createTable)\n",
    "## homepage\n",
    "html=getPage(\"http://news.baidu.com\")\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "Divs=bs.find_all('div')\n",
    "for d in Divs:\n",
    "    if d.get(\"id\")==\"pane-news\":\n",
    "        links=d.find_all(\"a\")\n",
    "        for a in links:\n",
    "            print(a)\n",
    "            content=getSubPage(a,longtxt=2)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "## sub page\n",
    "Times=int(sys.argv[1])\n",
    "HomePages=((\"http://finance.baidu.com\",2),\n",
    "    (\"http://guonei.news.baidu.com\",3),\n",
    "    (\"http://guoji.news.baidu.com\",3),\n",
    "    (\"http://shehui.news.baidu.com\",5),\n",
    "    (\"http://mil.news.baidu.com\",6),\n",
    "    (\"http://tech.baidu.com\",7),\n",
    "    (\"http://internet.baidu.com\",7))\n",
    "for page in HomePages:\n",
    "    url=page[0]\n",
    "    times=page[1]\n",
    "    if Times%times==0:\n",
    "        html=getPage(url)\n",
    "        bs=BeautifulSoup(html,\"html.parser\")\n",
    "        links=bs.find_all(\"a\")\n",
    "        for a in links:\n",
    "            content=getSubPage(a)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬取动态页面\n",
    "\n",
    "现在，很多网页通过使用JavaScript的Ajax技术实现了网页内容的动态改变。\n",
    "\n",
    "比如，一个很简单的例子，如果访问我们之前建立的web.py中的网址： http://127.0.0.1:5000/dynamic 上面会有一个按钮，每按一次，该页面就会向服务器再发一个请求，服务器收到请求后会随机产生一个数字，并返回。所以每一次点击按钮，网页内容都会动态改变。\n",
    "\n",
    "在碰到动态页面时，一种方法是精通JavaScript，并使用浏览器跟踪浏览器行为，分析JavaScript脚本，进而使用以上的方法模拟浏览器请求。但是这种方法非常复杂，很多时候JavaScript可能会复杂到一定程度，使得分析异常困难。\n",
    "\n",
    "而另外一种方法，即使用selenium直接调用浏览器，浏览器自动执行JavaScript，然后调用浏览器的HTML即可。这种方法非常方便，但是速度异常之慢。\n",
    "\n",
    "为了实现这一方法，我们首先要安装selenium：pip install selenium\n",
    "\n",
    "除此之外，还要安装浏览器的支持。几种常见的浏览器支持插件的下载地址：\n",
    "\n",
    "* firefox：https://github.com/mozilla/geckodriver/releases （对于Windows，下载后放到C:\\Windows下；对于Linux/Mac，放到/usr/local/bin下。）\n",
    "* chrome： https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "* safari： https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n",
    "* edge：   https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
    "\n",
    "安装完成之后，使用：\n",
    "```python\n",
    "from selenium import webdriver\n",
    "driver=webdriver.Firefox()\n",
    "## 做一些事情\n",
    "driver.close()\n",
    "```\n",
    "\n",
    "就可以打开网络驱动器，此时我们可以看到一个Firefox窗口被打开。同样注意的是，用完之后记得关掉。\n",
    "\n",
    "比如，淘宝的网页上，价格数据是动态加载的，我们可以使用如下代码找到价格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3356b4c84707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://detail.tmall.com/item.htm?id=524649065000&sku_properties=29112:97926\"\n",
    "driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "## 产品名\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "meta=bs.find_all(\"meta\")\n",
    "for m in meta:\n",
    "    if m.get(\"name\")==\"keywords\":\n",
    "        title=m.get(\"content\")\n",
    "## 价格\n",
    "pattern=re.compile(\"TShop.Setup\\(.+?\\);\")\n",
    "res=pattern.finditer(html.replace('\\r', '').replace('\\n', ''))\n",
    "for r in res:\n",
    "    json_text=r.group()\n",
    "    json_text=json_text[json_text.find('{'):-2]\n",
    "\n",
    "info=json.loads(json_text)\n",
    "print(title,info['detail']['defaultItemPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium另外一个更常用的功能是自动填充、点击。为了实现这一目的，首先需要能够找到相应的按钮、输入框等，有如下方法可以使用：\n",
    "\n",
    "* find_element_by_name\n",
    "* find_element_by_id\n",
    "* find_element_by_xpath\n",
    "* find_element_by_link_text\n",
    "* find_element_by_partial_link_text\n",
    "* find_element_by_tag_name\n",
    "* find_element_by_class_name\n",
    "* find_element_by_css_selector\n",
    "\n",
    "理解以上函数需要一些HTML、CSS、JavaScript的背景知识，我们在此不再详细讨论。在此，展示一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python爬虫:爬取拉勾网职位并分析 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd90jHmQgjRIXY7mIX539ZpwhFQM2OZCZvNMiSUIjcd_YfkSc8Cnn6iKsHvrmifn9NTHSnyL8YMIPQtOlBK4vjUwkY3UZ9ReBtA-0ZgXwy561hoQvhlM8xprtVo9-RYUDiAAEw80LtPIqexaAhVnIcPCEJB2Mn2lCVupnXWzMIwAA5Ae0f8bRARvQ..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python爬虫&可视化-舌尖上的“小龙虾” /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd90jHmQgjRIXY7mIX539ZpwhFQM2OZCZvNMiSUIjcd_YfkSc8Cnn6iKj-kLLyU3EIjchL23SuZDu6z4ZhRfZuF31setyDrI2QIYQ20oM16JkG0BmnKwnscj2hzOcebM5tbS4zB0kyvqjcO0CBWeSh3bc-lR_CaTGWpuTc3GUoBBZHm0CtGSSkZQQ..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python爬虫 | 一条高效的学习路径 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9gAjh8MJgnq3Mzykzf9tgIMS_sxdeItQSsYHwoxq1Fmdm_FsIoebRQwplAX_zKJc2nHGoOMhYq-yitSxruXv-Kx_Q1owGRniEr6VfCWQcc5sGQMi0p-AkjCrVPIETMbc3wOzOVLVeEN85jMbHJX5-7HXGLQVmIiovsw3Sjlqo_ZfdrgozfSg6bw..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python 爬虫分析豆瓣 TOP250 告诉你程序员业余该看什么书? /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9_YNPwnqTSiIWP1A7N5Io_X7Y47jRkv7NEmh7nxUoXhTaGPs_drsrMEK104jgW-MmlwWZCDyCTVc3KOQpkwNNZakmPenCyebshxpC4X0rOSLtBUAOJrESxH4aLHBrYq3uBiSIIWTkvmSJ0e-dVgxf-LnnMEgpfagou95UH1abI6CAFV_3u-a3OQ..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python爬虫,推荐一条高效的学习路径 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9gAjh8MJgnq3Mzykzf9tgIMS_sxdeItQSsYHwoxq1Fmdm_FsIoebRQyAkisoGe0drXRfLs2OFspv2CUSX2xsATPhZ4RVc4YK6lMQzpPCs-ZPlTbkXcf6ip4_txMfZ9t358ZKf4KXaPNmJuB9eoku_tcxYL8yydNZuswpjXRAdDLnvzHZXxeLdyg..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python爬虫多线程实战:爬取美桌高清壁纸 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9kgjph4KoFLeG097btFLUMbQa8bRj20PqLbrXydUZx4n4vqDerpdapvx0f5nhqfsrT-3ORoBvXOWw3UrZzdCBDNowPhrVJwuq5VwYtqkrHiLqDg1MrA-x5yYc5Ik9l3yE60eVbRmjVUE0jTfFMr-CllqS_kJDEzZUVYtnCEbARJzf3q8uRDTfkw..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "Python爬虫之selenium爬取国家自然科学基金数据库(二) /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9j7OW-k-bmXVdATayvZ-vIQL731NE4mUB1hl5U5EQF8bTWxfc054fbzALbK8XzhrcQToF-MU2PNBKZLsKwQMSIRIPrMG5c86U9-1Rs74_E5y2eT1BLsmnTHbFLjUAUjKHpe_zgyZm_uG7QlhiBCX4Ez3kzY_a2Q3jENXNNSaZOzrs46dn8Efgxg..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "python爬虫多线程实战:爬取美桌1080p壁纸图片 | 技术 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd952aMZ5rgbUjzT5Xb8IK4uWbZKj47Opja03CosfFG24Lnqb-n5n5LUKpD7fO1sh86YHClB0OjEak5hGWSTvTaF9onUql6SThq-7yPVVJkZqr98x3IYGu0zeinHRTAbrWhRbdPDeM7VV3N_xDmY9RvLZvapDSsMKagIjsrYYbWBpNFH6In5Ei3zg..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "我是如何零基础开始能写Python爬虫的 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd9inRqOeMFrWIxDntH0kslt7ZCv8E5220IbqJuPnwfJm5iNZuojVyU7-X9zbnOZAqDnJKVDhbjqNkjykqqvxlOnS33NuWVsJ9YCthpIerrKzOoqm6_CdHI_AX1KkUFh9nc3ZPXt64TaxMfbuz0HeD_KSmbVapXXLqEAA2UCPvHbn8YJSSFPgfogQ..&type=2&query=Python%E7%88%AC%E8%99%AB\n",
      "python爬虫28 | 你爬下的数据不分析一波可就亏了啊,使用python进行数据可视化 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS8gCVKMYhvEenxvx7aByKuCvUlyE6x3M81qXa8Fplpd97IYOzSwD0AaEK_N02HCnj-oTSid0VRHtf_4LPYtxIFKNDKAiYJMqEy1MH53nd35TJFDgybJSiTt0dgZQA9RP_aRTP25L0x1SKZWN6vb_jBZJY_iSVJUux7LBm6fecRdXTQPXEsboF5kkdyfvSNL2xEcscZSUXUKQrevHb4F_ybfRtmyIGw2VAg..&type=2&query=Python%E7%88%AC%E8%99%AB\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://weixin.sogou.com\"\n",
    "driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "driver.find_element_by_id('query').send_keys(\"Python爬虫\") #模仿填写搜索内容\n",
    "driver.find_element_by_class_name(\"swz\").click() #模仿点击搜索按钮\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='h3')\n",
    "for it in iterms:\n",
    "    print(it.a.text,it.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记：关于爬虫的一些补充\n",
    "\n",
    "关于爬虫，这里有一些需要补充的话。\n",
    "\n",
    "首先是关于爬虫的道德问题。实际上爬虫一直处在一个灰色地带：违法或者不违法，道德或者不道德。在此一些原则希望与大家分享：\n",
    "\n",
    "* 按照我国法律的相关规定，如果网页内容、API接口使用了加密，破解加密是违法行为。\n",
    "* 如果将爬取的数据商用，法律风险非常大\n",
    "* 如果网站提供了接口（比如豆瓣API），不使用接口而直接爬取网页也是不道德的。\n",
    "* 尽量爬取的速率不要太高，不要给他人的服务器造成太大负担。\n",
    "\n",
    "当然，还有一些技术出于时间所限、个人能力所限，我是没有讲到的，这其中很多都是与反爬虫有关。比如：\n",
    "\n",
    "* 如何破解验证码\n",
    "* 如何破解短信验证码\n",
    "* 消息队列\n",
    "* 并行爬虫\n",
    "\n",
    "等等。如果有需要，可以自行学习。\n",
    "\n",
    "最后，爬虫和反爬一直是相互伴生的两个技术，我们这里提供一些常用的反爬思路：\n",
    "\n",
    "* 如果request到的内容与自己使用浏览器看到的内容不符：\n",
    "    - 首先查看请求头部，最极端情况下，完全复制浏览器的请求头部，看看能不能得到相同的反应。\n",
    "    - 是否需要cookies登录？\n",
    "    - 是否有重定向？\n",
    "    - 仔细查看request得到的html，里面可能有玄机\n",
    "    - 实在不行，selenium\n",
    "* 如果request中的内容没有自己想要的信息：\n",
    "    - 是否是动态加载的？\n",
    "        * 对比request得到的HTML和浏览器的HTML是否不同，如果是不同的，可能时动态加载的\n",
    "        * 仔细分析JavaScript，或者浏览器发送的每一次请求\n",
    "        * 尝试直接发送动态请求\n",
    "    - 内容是否是加密的？\n",
    "        * 仔细分析加密方法\n",
    "* 爬取几个页面后被禁止访问：\n",
    "    - 可能爬取的频率太频繁\n",
    "    - 间隔时间小一点\n",
    "    - 使用代理池（比如该项目： https://github.com/SpiderClub/haipproxy ）\n",
    "    - 使用ADSL\n",
    "* 需要登录：\n",
    "    - 不频繁的登录通常使用cookies，模拟一次登录之后获得cookies即可\n",
    "    - 频繁的登录或者验证码，可能需要结合图像识别之类的方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
